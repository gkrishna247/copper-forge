{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem=\"\"\"\n",
    "Project Title Industrial Copper Modeling\n",
    "Skills take away From This Project Python scripting, Data\n",
    "Preprocessing,\n",
    "EDA, Streamlit\n",
    "Domain Manufacturing\n",
    "Problem Statement:\n",
    "The copper industry deals with less complex data related to sales and pricing.\n",
    "However, this data may suffer from issues such as skewness and noisy data, which\n",
    "can affect the accuracy of manual predictions. Dealing with these challenges manually\n",
    "can be time-consuming and may not result in optimal pricing decisions. A machine\n",
    "learning regression model can address these issues by utilizing advanced techniques\n",
    "such as data normalization, feature scaling, and outlier detection, and leveraging\n",
    "algorithms that are robust to skewed and noisy data.\n",
    "Another area where the copper industry faces challenges is in capturing the leads. A\n",
    "lead classification model is a system for evaluating and classifying leads based on\n",
    "how likely they are to become a customer . You can use the STATUS variable with\n",
    "WON being considered as Success and LOST being considered as Failure and\n",
    "remove data points other than WON, LOST STATUS values.\n",
    "The solution must include the following steps:\n",
    "1) Exploring skewness and outliers in the dataset.\n",
    "2) Transform the data into a suitable format and perform any necessary cleaning\n",
    "and pre-processing steps.\n",
    "3) ML Regression model which predicts continuous variable ‘Selling_Price’.\n",
    "4) ML Classification model which predicts Status: WON or LOST.\n",
    "5) Creating a streamlit page where you can insert each column value and you will\n",
    "get the Selling_Price predicted value or Status(Won/Lost)\n",
    "Data: Data-set\n",
    "About the Data:\n",
    "1. `id`: This column likely serves as a unique identifier for each transaction or item,\n",
    "which can be useful for tracking and record-keeping.\n",
    "2. `item_date`: This column represents the date when each transaction or item was\n",
    "recorded or occurred. It's important for tracking the timing of business activities.\n",
    "3. `quantity tons`: This column indicates the quantity of the item in tons, which is\n",
    "essential for inventory management and understanding the volume of products sold or\n",
    "produced.\n",
    "4. `customer`: The \"customer\" column refers to the name or identifier of the customer\n",
    "who either purchased or ordered the items. It's crucial for maintaining customer\n",
    "relationships and tracking sales.\n",
    "5. `country`: The \"country\" column specifies the country associated with each\n",
    "customer. This information can be useful for understanding the geographic distribution\n",
    "of customers and may have implications for logistics and international sales.\n",
    "6. `status`: The \"status\" column likely describes the current status of the transaction\n",
    "or item. This information can be used to track the progress of orders or transactions,\n",
    "such as \"Draft\" or \"Won.\"\n",
    "7. `item type`: This column categorizes the type or category of the items being sold or\n",
    "produced. Understanding item types is essential for inventory categorization and\n",
    "business reporting.\n",
    "8. `application`: The \"application\" column defines the specific use or application of\n",
    "the items. This information can help tailor marketing and product development efforts.\n",
    "9. `thickness`: The \"thickness\" column provides details about the thickness of the\n",
    "items. It's critical when dealing with materials where thickness is a significant factor,\n",
    "such as metals or construction materials.\n",
    "10. `width`: The \"width\" column specifies the width of the items. It's important for\n",
    "understanding the size and dimensions of the products.\n",
    "11. `material_ref`: This column appears to be a reference or identifier for the material\n",
    "used in the items. It's essential for tracking the source or composition of the products.\n",
    "12. `product_ref`: The \"product_ref\" column seems to be a reference or identifier for\n",
    "the specific product. This information is useful for identifying and cataloging products\n",
    "in a standardized way.\n",
    "13. `delivery date`: This column records the expected or actual delivery date for each\n",
    "item or transaction. It's crucial for managing logistics and ensuring timely delivery to\n",
    "customers.\n",
    "14. `selling_price`: The \"selling_price\" column represents the price at which the items\n",
    "are sold. This is a critical factor for revenue generation and profitability analysis.\n",
    "Approach:\n",
    "1) Data Understanding: Identify the types of variables (continuous, categorical)\n",
    "and their distributions. Some rubbish values are present in ‘Material_Reference’\n",
    "which starts with ‘00000’ value which should be converted into null. Treat\n",
    "reference columns as categorical variables. INDEX may not be useful.\n",
    "2) Data Preprocessing:\n",
    "● Handle missing values with mean/median/mode.\n",
    "● Treat Outliers using IQR or Isolation Forest from sklearn library.\n",
    "● Identify Skewness in the dataset and treat skewness with appropriate\n",
    "data transformations, such as log transformation(which is best suited to\n",
    "transform target variable-train, predict and then reverse transform it back\n",
    "to original scale eg:dollars), boxcox transformation, or other techniques,\n",
    "to handle high skewness in continuous variables.\n",
    "● Encode categorical variables using suitable techniques, such as one-hot\n",
    "encoding, label encoding, or ordinal encoding, based on their nature and\n",
    "relationship with the target variable.\n",
    "3) EDA: Try visualizing outliers and skewness(before and after treating skewness)\n",
    "using Seaborn’s boxplot, distplot, violinplot.\n",
    "4) Feature Engineering: Engineer new features if applicable, such as aggregating\n",
    "or transforming existing features to create more informative representations of\n",
    "the data. And drop highly correlated columns using SNS HEATMAP.\n",
    "5) Model Building and Evaluation:\n",
    "● Split the dataset into training and testing/validation sets.\n",
    "● Train and evaluate different classification models, such as\n",
    "ExtraTreesClassifier, XGBClassifier, or Logistic Regression, using\n",
    "appropriate evaluation metrics such as accuracy, precision, recall, F1\n",
    "score, and AUC curve.\n",
    "● Optimize model hyperparameters using techniques such as\n",
    "cross-validation and grid search to find the best-performing model.\n",
    "● Interpret the model results and assess its performance based on the\n",
    "defined problem statement.\n",
    "● Same steps for Regression modelling.(note: dataset contains more noise\n",
    "and linearity between independent variables so itll perform well only with\n",
    "tree based models)\n",
    "6) Model GUI: Using streamlit module, create interactive page with\n",
    "(1) task input( Regression or Classification) and\n",
    "(2) create an input field where you can enter each column value except\n",
    "‘Selling_Price’ for regression model and except ‘Status’ for classification\n",
    "model.\n",
    "(3) perform the same feature engineering, scaling factors, log/any\n",
    "transformation steps which you used for training ml model and predict this new\n",
    "data from streamlit and display the output.\n",
    "7) Tips: Use pickle module to dump and load models such as encoder(onehot/\n",
    "label/ str.cat.codes /etc), scaling models(standard scaler), ML models. First fit\n",
    "and then transform in separate line and use transform only for unseen data\n",
    "Eg: scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "scaler.transform(X_train)\n",
    "scaler.transform(X_test_new) #unseen data\n",
    "The learning outcomes of this project are:\n",
    "1. Developing proficiency in Python programming language and its data analysislibraries such as Pandas, NumPy, Matplotlib, Seaborn, Scikit-learn, andStreamlit.\n",
    "2. Gaining experience in data preprocessing techniques such as handling missingvalues, outlier detection, and data normalization to prepare data for machinelearning modeling.\n",
    "3. Understanding and visualizing the data using EDA techniques such asboxplots, histograms, and scatter plots.\n",
    "4. Learning and applying advanced machine learning techniques such asregression and classification to predict continuous and binary target variables,respectively.\n",
    "5. Building and optimizing machine learning models using appropriate evaluationmetrics and techniques such as cross-validation and grid search.\n",
    "6. Experience in feature engineering techniques to create new informativerepresentations of the data.\n",
    "7. Developing a web application using the Streamlit module to showcase themachine learning models and make predictions on new data.\n",
    "8. Understanding the challenges and best practices in the manufacturing domainand how machine learning can help solve them.\n",
    "Overall, this project will equip you with practical skills and experience in data analysis,\n",
    "machine learning modeling, and creating interactive web applications, and provide you\n",
    "with a solid foundation to tackle real-world problems in the manufacturing domain.\n",
    "Project Evaluation metrics:\n",
    "● You are supposed to write a code in a modular fashion (in functional blocks)\n",
    "● Maintainable: It can be maintained, even as your codebase grows.\n",
    "● Portable: It works the same in every environment (operating system)\n",
    "● You have to maintain your code on GitHub.(Mandatory)\n",
    "● You have to keep your GitHub repo public so that anyone can check your\n",
    "code.(Mandatory)\n",
    "● Proper readme file you have to maintain for any project\n",
    "development(Mandatory)\n",
    "● You should include basic workflow and execution of the entire project in the\n",
    "readme file on GitHub (Mandatory)\n",
    "● Follow the coding standards: https://www.python.org/dev/peps/pep-0008/\n",
    "● You need to Create a Demo video of your working model and post in\n",
    "LinkedIn(Mandatory)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "path=\"Copper_Set.xlsx\"\n",
    "df = pd.read_excel(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the data\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the number of unique values in each features\n",
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data types\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the object data type to numeric\n",
    "df['quantity tons'] = pd.to_numeric(df['quantity tons'], errors='coerce')\n",
    "df['item_date_1'] = pd.to_datetime(df['item_date'], format='%Y%m%d', errors='coerce').dt.date\n",
    "df['delivery date_1'] = pd.to_datetime(df['delivery date'], format='%Y%m%d', errors='coerce').dt.date\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some rubbish values are present in ‘Material_ref’ which starts with ‘00000’ value which should be converted into null\n",
    "df['material_ref'] = df['material_ref'].apply(lambda x: np.nan if str(x).startswith('00000') else x)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the index column is not useful so we can drop column 'id'\n",
    "# and also drop the material_ref column as it has many missing values\n",
    "df.drop(['id', 'material_ref'], axis=1, inplace=True)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the data to check the summary statistics\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantity and selling price values are not below 0. so we convert to null for below 0 values.\n",
    "df['quantity tons'] = df['quantity tons'].apply(lambda x: np.nan if x<0 else x)\n",
    "df['selling_price'] = df['selling_price'].apply(lambda x: np.nan if x<0 else x)\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle missing values with mean/median/mode\n",
    "# for numerical columns we will use median and for object columns we will use mode\n",
    "# Object datatype using mode\n",
    "df['item_date'] = df['item_date'].fillna(df['item_date'].mode().iloc[0])\n",
    "df['item_date_1'] = df['item_date_1'].fillna(df['item_date_1'].mode().iloc[0])\n",
    "df['status'] = df['status'].fillna(df['status'].mode().iloc[0])\n",
    "df['delivery date'] = df['delivery date'].fillna(df['delivery date'].mode().iloc[0])\n",
    "df['delivery date_1'] = df['delivery date_1'].fillna(df['delivery date_1'].mode().iloc[0])\n",
    "\n",
    "# Numerical datatype using median\n",
    "df['quantity tons'] = df['quantity tons'].fillna(df['quantity tons'].median())\n",
    "df['customer'] = df['customer'].fillna(df['customer'].median())\n",
    "df['country'] = df['country'].fillna(df['country'].median())\n",
    "df['application'] = df['application'].fillna(df['application'].median())\n",
    "df['thickness'] = df['thickness'].fillna(df['thickness'].median())\n",
    "df['selling_price'] = df['selling_price'].fillna(df['selling_price'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data types\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['item type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the categorical columns to numerical using label encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['status'] = le.fit_transform(df['status'])\n",
    "df['item type'] = le.fit_transform(df['item type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the outliers and skewness using seaborn's boxplot, distplot, violinplot\n",
    "# find outliers using boxplot\n",
    "# skewness using distplot and violinplot\n",
    "def plot(df, column):\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(1,3,1)\n",
    "    sns.boxplot(data=df, x=column)\n",
    "    plt.title(f'Box Plot for {column}')\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    sns.histplot(data=df, x=column, kde=True, bins=50)\n",
    "    plt.title(f'Distribution Plot for {column}')\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    sns.violinplot(data=df, x=column)\n",
    "    plt.title(f'Violin Plot for {column}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for outliers and skewness in the data\n",
    "for i in ['quantity tons', 'customer', 'country', 'item type', 'application', 'thickness', 'width', 'selling_price']:\n",
    "    plot(df, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantity tons, customer, country, thickness, width, selling_price are right skewed\n",
    "# apply log transformation to reduce the skewness \n",
    "df['quantity tons_log'] = np.log1p(df['quantity tons'])\n",
    "df['customer_log'] = np.log1p(df['customer'])\n",
    "df['country_log'] = np.log1p(df['country'])\n",
    "df['thickness_log'] = np.log1p(df['thickness'])\n",
    "df['width_log'] = np.log1p(df['width'])\n",
    "df['selling_price_log'] = np.log1p(df['selling_price'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for outliers and skewness in the data\n",
    "for i in ['quantity tons_log', 'customer_log', 'country_log', 'item type', 'application', 'thickness_log', 'width_log', 'selling_price_log']:\n",
    "    plot(df, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers Handling - Interquartile Range (IQR) method\n",
    "# calculate the IQR\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the outliers\n",
    "df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the data\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for outliers and skewness in the data\n",
    "for i in ['quantity tons_log', 'customer_log', 'country_log', 'item type', 'application', 'thickness_log', 'width_log', 'selling_price_log']:\n",
    "    plot(df, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the original columns\n",
    "df.drop(['quantity tons', 'customer', 'country', 'thickness', 'width', 'selling_price'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the data\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data types\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "df.to_csv('Copper_Set_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the data\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data types\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check for highly correlated columns using sns heatmap\n",
    "columns = ['item_date', 'status', 'item type', 'application', 'product_ref', 'delivery date', 'quantity tons_log', 'customer_log', 'country_log', 'thickness_log', 'width_log', 'selling_price_log']\n",
    "corr = df[columns].corr()\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no highly correlated columns are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "item_date\tstatus\titem type\tapplication\tproduct_ref\tdelivery date\titem_date_1\tdelivery date_1\tquantity tons_log\tcustomer_log\tcountry_log\tthickness_log\twidth_log\tselling_price_log\n",
    "0\t20210401.0\t7\t5\t10.0\t1670798778\t20210701.0\t2021-04-01\t2021-07-01\t4.010077\t17.221905\t3.367296\t1.098612\t7.313887\t6.751101\n",
    "7\t20210401.0\t7\t5\t41.0\t611993\t20210101.0\t2021-04-01\t2021-01-01\t4.739589\t17.228050\t3.496508\t0.518794\t7.107425\t6.837333\n",
    "8\t20210401.0\t7\t3\t10.0\t1668701376\t20210701.0\t2021-04-01\t2021-07-01\t4.249521\t17.228025\t3.663562\t0.470004\t7.151485\t7.218177\n",
    "11\t20210401.0\t7\t5\t41.0\t611993\t20210101.0\t2021-04-01\t2021-01-01\t4.744894\t17.228050\t3.496508\t0.438255\t7.107425\t6.861711\n",
    "12\t20210401.0\t7\t5\t10.0\t164141591\t20210701.0\t2021-04-01\t2021-07-01\t3.350344\t17.222210\t4.369448\t0.559616\t7.131699\t7.002156\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the data columns\n",
    "# drop item_date and delivery date columns\n",
    "df.drop(['item_date', 'delivery date'], axis=1, inplace=True)\n",
    "# rename the columns\n",
    "df.rename(columns={'item_date_1':'item_date', 'delivery date_1':'delivery_date', 'quantity tons_log':'quantity_tons', 'customer_log':'customer', 'country_log':'country', 'thickness_log':'thickness', 'width_log':'width', 'selling_price_log':'selling_price'}, inplace=True)\n",
    "# split the dates into day, month and year\n",
    "df['item_date'] = pd.to_datetime(df['item_date'])\n",
    "df['delivery_date'] = pd.to_datetime(df['delivery_date'])\n",
    "df['item_day'] = df['item_date'].dt.day\n",
    "df['item_month'] = df['item_date'].dt.month\n",
    "df['item_year'] = df['item_date'].dt.year\n",
    "df['delivery_day'] = df['delivery_date'].dt.day\n",
    "df['delivery_month'] = df['delivery_date'].dt.month\n",
    "df['delivery_year'] = df['delivery_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets handle the wrong delivery date values\n",
    "# The delivery date should be greater than the item date so we split the data into two parts such as correct and wrong delivery date values\n",
    "# correct delivery date values are greater than the item date\n",
    "# wrong delivery date values are less than the item date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the data\n",
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column to check the delivery date\n",
    "df1['delivery_check'] = np.where(df1['delivery_date'] > df1['item_date'], 'Correct', 'Wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the value counts\n",
    "df1['delivery_check'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into correct and wrong delivery date values\n",
    "df_correct = df1[df1['delivery_check'] == 'Correct']\n",
    "df_wrong = df1[df1['delivery_check'] == 'Wrong']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correct.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first train a model to predict the correct delivery date values using the correct delivery date values that we have in the data and then predict the wrong delivery date values using the trained model\n",
    "# we can use regression model to predict the correct delivery date values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's first create a ML Regression model which predicts the continuous variable 'Selling_Price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into features and target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
